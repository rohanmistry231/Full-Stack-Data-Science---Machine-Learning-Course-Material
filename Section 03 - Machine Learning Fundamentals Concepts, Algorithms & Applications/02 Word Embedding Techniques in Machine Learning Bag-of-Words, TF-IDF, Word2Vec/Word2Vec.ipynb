{"cells":[{"cell_type":"markdown","metadata":{"id":"qI3-Qh_2pXbW"},"source":["# Word to Vector(W2V)"]},{"cell_type":"markdown","metadata":{"id":"u-ZE1HszpXbw"},"source":["- W2V is a state of the art technology.\n","- W2V takes only a __word__ into consideration and __NOT a sentence__ and converts it into a d-dimensional vector.\n","- Similar words have vectors close to each other.\n","- For example, let there be two similar words $w_{1}$ and $w_{2}$. Their vectors $v_{1}$ and $v_{2}$ will be close to each other.\n","- Non-similar words have vectors far away from each other.\n","- Similar words have vectors parallel to each other. Such as the words 'King' and 'Man' have their vectors parallel to each other.\n","- It learns and maintains relationship between words i.e it considers the semantic meaning of words. This is depicted below:\n","\n","![alt text](https://www.tensorflow.org/images/linear-relationships.png)"]},{"cell_type":"markdown","metadata":{"id":"iMNXY_mipXb2"},"source":["- **Larger the dimensionality** of a vector, **more information rich** it becomes. Generally the dimensionality of a vector is kept high around 50, 200, 300 or more as per the requirements.\n","- Large document corpus can have high dimensionality vectors.\n","- At its core, W2V uses **neighbourhood** of a word so that similar words are close to each other and non-similar words are away from each other."]}],"metadata":{"kernelspec":{"display_name":"Python [conda env:Anaconda3]","language":"python","name":"conda-env-Anaconda3-py"},"language_info":{"codemirror_mode":{"name":"ipython","version":3},"file_extension":".py","mimetype":"text/x-python","name":"python","nbconvert_exporter":"python","pygments_lexer":"ipython3","version":"3.6.6"},"colab":{"name":"5. Word2Vec.ipynb","provenance":[]}},"nbformat":4,"nbformat_minor":0}