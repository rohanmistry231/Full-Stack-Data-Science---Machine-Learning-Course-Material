{"cells":[{"cell_type":"markdown","metadata":{"id":"kaYNMVqj5yxo"},"source":["## Introduction\n","\n","This is an analysis of Wikipedia comments to create models that identify various types of toxic comments. There is a lot of racist content and swear words in the dataset and some of it will pop up in the analysis. \n","\n","The Conversation AI team, a research initiative founded by Jigsaw and Google (both a part of Alphabet) are working on tools to help improve online conversation. One area of focus is the study of negative online behaviors, like toxic comments (i.e. comments that are rude, disrespectful or otherwise likely to make someone leave a discussion). So far they’ve built a range of publicly available models served through the Perspective API, including toxicity. But the current models still make errors, and they don’t allow users to select which types of toxicity they’re interested in finding (e.g. some platforms may be fine with profanity, but not with other types of toxic content).\n","\n","we need to build a multi-headed model that’s capable of detecting different types of of toxicity like threats, obscenity, insults, and identity-based hate better than Perspective’s current models. You’ll be using a dataset of comments from Wikipedia’s talk page edits. Improvements to the current model will hopefully help online discussion become more productive and respectful.\n","\n","## Dataset\n","The dataset taken from kaggle : https://www.kaggle.com/competitions/jigsaw-toxic-comment-classification-challenge/data\n","\n","## Dataset Description\n","You are provided with a large number of Wikipedia comments which have been labeled by human raters for toxic behavior. The types of toxicity are:\n","\n","1.toxic\n","2.severe_toxic\n","3.obscene\n","4.threat\n","5.insult\n","6.identity_hate\n","\n","we must create a model which predicts a probability of each type of toxicity for each comment.\n"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"It7csY1f5yxs"},"outputs":[],"source":["import pandas as pd\n","import pickle\n","import numpy as np\n","import nltk\n","from nltk.corpus import stopwords\n","import keras\n","import time\n","import matplotlib.pyplot as plt\n","import seaborn as sns\n","import re\n","import string\n","from collections import namedtuple\n","from sklearn.preprocessing import StandardScaler\n","from sklearn.model_selection import cross_val_score\n","from sklearn.model_selection import StratifiedKFold\n","from sklearn.model_selection import train_test_split \n","from sklearn.metrics import f1_score\n","from sklearn.feature_extraction.text import TfidfVectorizer\n","from scipy import sparse\n","from sklearn.pipeline import make_union\n","from sklearn.svm import LinearSVC\n","from sklearn.naive_bayes import MultinomialNB\n","from sklearn.linear_model import LogisticRegression\n","import lightgbm as lgb\n","from sklearn.metrics import confusion_matrix"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"eZ7kG9xl9hWo"},"outputs":[],"source":["# Global random state and k-fold strategy \n","seed = 42\n","k = 5\n","cv = StratifiedKFold(n_splits=5, random_state=seed,shuffle=True)"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"xN7yppOd9hWq"},"outputs":[],"source":["def lgb_f1_score(y_hat, data):\n","    # https://stackoverflow.com/questions/49774825/python-lightgbm-cross-validation-how-to-use-lightgbm-cv-for-regression\n","    y_true = data.get_label()\n","    y_hat = np.round(y_hat) \n","    return 'f1', f1_score(y_true, y_hat), True"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"keYiW4Ws5yx2"},"outputs":[],"source":["start = time.time()\n","def print_time(start):\n","    time_now = time.time() - start \n","    minutes = int(time_now / 60)\n","    seconds = int(time_now % 60)\n","    if seconds < 10:\n","        print('Elapsed time was %d:0%d.' % (minutes, seconds))\n","    else:\n","        print('Elapsed time was %d:%d.' % (minutes, seconds))"]},{"cell_type":"markdown","metadata":{"id":"zkLjce-D9hWu"},"source":["## Feature Engineering "]},{"cell_type":"code","execution_count":null,"metadata":{"id":"S2_DmJ5f9hWv"},"outputs":[],"source":["def feature_engineering(df, sparse=0): \n","    \n","    # Comment length\n","    df['length'] = df.comment_text.apply(lambda x: len(x))\n","    \n","\n","    # Capitalization percentage\n","    def pct_caps(s):\n","        return sum([1 for c in s if c.isupper()]) / (sum(([1 for c in s if c.isalpha()])) + 1)\n","    df['caps'] = df.comment_text.apply(lambda x: pct_caps(x))\n","\n","    # Mean Word length \n","    def word_length(s):\n","        s = s.split(' ')\n","        return np.mean([len(w) for w in s if w.isalpha()])\n","    df['word_length'] = df.comment_text.apply(lambda x: word_length(x))\n","\n","    # Average number of exclamation points \n","    df['exclamation'] = df.comment_text.apply(lambda s: len([c for c in s if c == '!']))\n","\n","    # Average number of question marks \n","    df['question'] = df.comment_text.apply(lambda s: len([c for c in s if c == '?']))\n","    \n","    # Normalize\n","    for label in ['length', 'caps', 'word_length', 'question', 'exclamation']:\n","        minimum = df[label].min()\n","        diff = df[label].max() - minimum\n","        df[label] = df[label].apply(lambda x: (x-minimum) / (diff))\n","\n","    # Strip IP Addresses\n","    ip = re.compile('(([2][5][0-5]\\.)|([2][0-4][0-9]\\.)|([0-1]?[0-9]?[0-9]\\.)){3}'\n","                    +'(([2][5][0-5])|([2][0-4][0-9])|([0-1]?[0-9]?[0-9]))')\n","    def strip_ip(s, ip):\n","        try:\n","            found = ip.search(s)\n","            return s.replace(found.group(), ' ')\n","        except:\n","            return s\n","\n","    df.comment_text = df.comment_text.apply(lambda x: strip_ip(x, ip))\n","    \n","    return df\n","\n","def merge_features(comment_text, data, engineered_features):\n","    new_features = sparse.csr_matrix(df[engineered_features].values)\n","    if np.isnan(new_features.data).all():\n","        new_features.data = np.nan_to_num(new_features.data)\n","    return sparse.hstack([comment_text, new_features])"]},{"cell_type":"markdown","metadata":{"id":"o0MDj8c29hWy"},"source":["## Loading Dataset"]},{"cell_type":"code","execution_count":null,"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"elapsed":815582,"status":"ok","timestamp":1664790177182,"user":{"displayName":"akhilvydyula01 dump","userId":"04060569174740840718"},"user_tz":-330},"id":"xziVkm90HBFK","outputId":"b21f50f1-a44c-4b2d-a9f6-155088e2d7aa"},"outputs":[{"name":"stdout","output_type":"stream","text":["Mounted at /content/drive\n"]}],"source":["from google.colab import drive\n","drive.mount('/content/drive')"]},{"cell_type":"code","execution_count":null,"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"elapsed":56036,"status":"ok","timestamp":1664790233198,"user":{"displayName":"akhilvydyula01 dump","userId":"04060569174740840718"},"user_tz":-330},"id":"7pgN8VDP9hW2","outputId":"e42d61cd-d8b4-4b8f-fee0-8ebd5a79ca0b"},"outputs":[{"name":"stderr","output_type":"stream","text":["/usr/local/lib/python3.7/dist-packages/numpy/core/fromnumeric.py:3441: RuntimeWarning: Mean of empty slice.\n","  out=out, **kwargs)\n"]},{"name":"stdout","output_type":"stream","text":["Training labels:\n","['toxic', 'severe_toxic', 'obscene', 'threat', 'insult', 'identity_hate']\n","(159571, 6)\n","\n","Training data\n","['comment_text', 'length', 'caps', 'word_length', 'exclamation', 'question']\n","(159571, 6)\n","\n","Submission data\n","['comment_text', 'length', 'caps', 'word_length', 'exclamation', 'question']\n","(153164, 6)\n","['length', 'caps', 'word_length', 'exclamation', 'question']\n"]}],"source":["# Reset data and create holdout set. \n","\n","df = pd.read_csv('/content/drive/MyDrive/UDEMY_PROJECTS/Project_4_Toxic_Comments_Classification/train.csv')\n","targets = list(df.columns[2:])\n","df_targets = df[targets].copy()\n","\n","df_sub = pd.read_csv('/content/drive/MyDrive/UDEMY_PROJECTS/Project_4_Toxic_Comments_Classification/test.csv', dtype={'id': object}, na_filter=False)\n","\n","submission = pd.DataFrame()\n","submission['id'] = df_sub.id.copy()\n","\n","# Feature Engineering\n","df = feature_engineering(df)\n","df_sub = feature_engineering(df_sub)\n","\n","print('Training labels:')\n","print(list(df_targets.columns))\n","print(df_targets.shape)\n","\n","print('\\nTraining data')\n","df.drop(list(df_targets.columns), inplace=True, axis=1)\n","df.drop('id', inplace=True, axis=1)\n","print(list(df.columns))\n","print(df.shape)\n","\n","\n","print('\\nSubmission data')\n","df_sub.drop('id', inplace=True, axis=1)\n","print(list(df_sub.columns))\n","print(df_sub.shape)\n","\n","toxic_rows = df_targets.sum(axis=1)\n","toxic_rows = (toxic_rows > 0)\n","targets.append('any_label')\n","df_targets['any_label'] = toxic_rows.astype(int)\n","\n","new_features = list(df.columns[1:])\n","print(new_features)\n","\n","from sklearn.model_selection import train_test_split\n","df, holdout, df_targets, holdout_targets = train_test_split(df, df_targets, test_size=0.2, random_state=seed)"]},{"cell_type":"code","execution_count":null,"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"elapsed":40,"status":"ok","timestamp":1664790233200,"user":{"displayName":"akhilvydyula01 dump","userId":"04060569174740840718"},"user_tz":-330},"id":"umyhbq6U9hW7","outputId":"ebe653fc-8d91-4767-cb31-1fd3a6b3ade8"},"outputs":[{"data":{"text/plain":["['length', 'caps', 'word_length', 'exclamation', 'question']"]},"execution_count":8,"metadata":{},"output_type":"execute_result"}],"source":["new_features"]},{"cell_type":"markdown","metadata":{"id":"-yBHO_Vc9hW9"},"source":["## Multilabel Function "]},{"cell_type":"code","execution_count":null,"metadata":{"id":"4QrwxlwC9hW-"},"outputs":[],"source":["from sklearn.base import clone\n","#todo \n","# Weights for \n","def multi_cv(model, data, labels, k=5, nb_features=False):\n","    cv = StratifiedKFold(n_splits=k, random_state=None)\n","    # Creating NB features just once from any_label has about the same \n","    # performance as individual labels with faster speed. \n","    def log_count_ratio(x, y):\n","        x = sparse.csr_matrix(x)\n","        # WARNING: Some scipy modules use indexes that start at 1! \n","        # You need to add 1 to an index when performing operations on a csr_matrix \n","\n","        p = abs(x[np.where(y==1)].sum(axis=0))\n","        p = p + 1\n","        p = p / np.sum(p)\n","\n","        q = abs(x[np.where(y==0)].sum(axis=0))\n","        q = q + 1\n","        q = q / np.sum(q)\n","\n","        return np.log(p/q)\n","    \n","    # Labels must be in a dataframe\n","    scores = []\n","    r_values = []\n","    for label in labels.columns:\n","        if nb_features:\n","            r = log_count_ratio(data, labels[label])\n","            r_values.append(r)\n","            data = data.multiply(r)\n","            if np.isnan(data.data).any():\n","                data.data = np.nan_to_num(data.data)\n","        score = np.mean(cross_val_score(clone(model), data, labels[label], scoring='f1', cv=cv))\n","        print(label + ' f1 score: %.4f' % score)\n","        scores.append(score)\n","    print('Average (excluding any) f1 score: %.4f' % np.mean(scores[:-1]))\n","    if nb_features:\n","        return scores, r_values\n","    else:\n","        return scores\n","\n","# training_comments.data = np.nan_to_num(training_comments.data)\n","\n","# model = LinearSVC()\n","# _ = multi_cv(model, training_comments, df_targets, nb_features=True)"]},{"cell_type":"markdown","metadata":{"id":"vASAxlsB9hXB"},"source":["## NB Feature Transformer \n","\n","This is the primary method that I will use for the NB-SVM models, but I've left other code in to use as a reference. "]},{"cell_type":"code","execution_count":null,"metadata":{"id":"INM6sdyz9hXC"},"outputs":[],"source":["class NBFeatures:\n","    def __init__(self, epsilon=1, sparse=True):\n","        # How much influence NB features have \n","        if not epsilon > 0 and epsilon <= 1:\n","            raise Exception(\"Invalid Epsilon value. Must be greater than zero and less than or equal to one.\")\n","        self.epsilon = epsilon\n","        self.r = None\n","    \n","    def log_count_ratio(self, x, y):\n","        x = sparse.csr_matrix(x)\n","        # WARNING: Some scipy authors fall in the \"index starts at 1\" camp\n","        # You need to add 1 to an index when performing operations on a csr_matrix \n","        p = abs(x[np.where(y==1)].sum(axis=0))\n","        p = p + 1\n","        p = p / np.sum(p)\n","        q = abs(x[np.where(y==0)].sum(axis=0))\n","        q = q + 1\n","        q = q / np.sum(q)\n","        return np.log(p/q)\n","    \n","    def fit(self, x, y):\n","        self.r = self.log_count_ratio(x, y)\n","    \n","    def transform(self, x):\n","        if self.r == None: \n","            raise Exception(\"Model not fit, can't transform.\")\n","        transformed = x.multiply(self.r)\n","        return x.multiply(1-self.epsilon) + transformed.multiply(self.epsilon)\n","        #return np.multiply(x, self.r)\n","    \n","    def fit_transform(self, x, y):\n","        self.r = self.log_count_ratio(x, y)\n","        return self.transform(x, y)\n","\n","\n","#nb_trans = NBFeatures(0.5)\n","#new = nb_trans.fit_transform(training_comments, np.array(df_targets.iloc[:,-1]))\n","#nb_trans.r.shape"]},{"cell_type":"markdown","metadata":{"id":"IV0rTZ_D9hXF"},"source":["A separate helper function to calculate the log count ratio that can be used for experimentation. "]},{"cell_type":"code","execution_count":null,"metadata":{"id":"8Td30g249hXH"},"outputs":[],"source":["def log_count_ratio(x, y):\n","    x = sparse.csr_matrix(x)\n","    # WARNING: Some scipy authors fall in the \"index starts at 1\" camp\n","    # You need to add 1 to an index when performing operations on a csr_matrix \n","\n","    p = abs(x[np.where(y==1)].sum(axis=0))\n","    p = p + 1\n","    p = p / np.sum(p)\n","\n","    q = abs(x[np.where(y==0)].sum(axis=0))\n","    q = q + 1\n","    q = q / np.sum(q)\n","\n","    return np.log(p/q)\n"]},{"cell_type":"markdown","metadata":{"id":"CFWy2yiR9hXI"},"source":["# Vectorizing text"]},{"cell_type":"markdown","metadata":{"id":"CA6DJWQE9hXJ"},"source":["It's necessary to vectorize text before inputting into machine learning models. This is a process of translating string data into numerical data that the computer can better understand. Vectorized data is usually sparse, with an array where the features contain either word counts or another way of representing the occurance of characters or words in a string. This is done with a vectorizer object, which stores a dictionary of characters or words and their associated integer representation, along with relevant statistics if applicable. \n","\n","The strategy I'm going to use here is term frequency - inverse document frequency. This is a statistic that describes the usefulness of a string of characters by looking at the frequency that it occurs in an individual document (here, a single comment) and the inverse of its frequency in all of the documents in the dataset. \n","\n","That means that a word that is used frequently in a comment in this dataset, but that few comments in the dataset feature, is probably useful to the model. But a string that occurs in nearly every document is almost useless. "]},{"cell_type":"code","execution_count":null,"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"elapsed":10787,"status":"ok","timestamp":1664790243972,"user":{"displayName":"akhilvydyula01 dump","userId":"04060569174740840718"},"user_tz":-330},"id":"TGoGjh6b5y0i","outputId":"5f9713ca-ebf5-4374-82a7-f8351d914695"},"outputs":[{"name":"stdout","output_type":"stream","text":["Elapsed time was 0:11.\n","(127656, 10000)\n"]}],"source":["start = time.time()\n","comment_vector = TfidfVectorizer(max_features=10000, analyzer='word', #ngram_range=(2, 6), \n","                                 stop_words='english')\n","training_comments = comment_vector.fit_transform(df.comment_text)\n","holdout_comments = comment_vector.transform(holdout.comment_text)\n","submission_comments = comment_vector.transform(df_sub.comment_text)\n","print_time(start)\n","\n","print(training_comments.shape)"]},{"cell_type":"markdown","metadata":{"id":"bHlW4YQw9hXM"},"source":["One of the most important parameters to tune in this problem is the number of features and the n_gram range in the TF-IDF vectorizer, as well as choosing whether to analyze the sequences by characters or words. Analyzing by single words initially gives very poor performance, possibly because slang words and misspellings reduce the frequency of individual bad words.  "]},{"cell_type":"markdown","metadata":{"id":"NKhg8Ro69hXN"},"source":["This is a simple function to play with reducing class imbalance. "]},{"cell_type":"code","execution_count":null,"metadata":{"id":"piLsEq8H9hXO"},"outputs":[],"source":["# This is just experimental to learn about the behavior of models with imbalanced classes. \n","\n","from numpy.random import sample\n","\n","def imbalance_reduction(p, y):\n","    \"\"\"\n","    For multilabel problems, keeps all rows with a \n","    positive label and returns p% of data where label is zero. \n","    \"\"\"\n","    #reduce y\n","    y = np.sum(y, axis=1)\n","    p = 1-p\n","    keep_index = sample(len(y))\n","    keep_index = keep_index + y\n","    keep_index[keep_index>=p] = 1\n","    keep_index[keep_index<p] = 0\n","\n","    return np.where(keep_index==1)"]},{"cell_type":"markdown","metadata":{"id":"YACpDA3D9hXP"},"source":["# Benchmarks"]},{"cell_type":"markdown","metadata":{"id":"Nka5cyQM9hXQ"},"source":["### Logistic Regression"]},{"cell_type":"markdown","metadata":{"id":"4bOGcC1S9hXQ"},"source":["With 10,000 vectorized features, but without engineered features. "]},{"cell_type":"code","execution_count":null,"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"elapsed":55813,"status":"ok","timestamp":1664790299773,"user":{"displayName":"akhilvydyula01 dump","userId":"04060569174740840718"},"user_tz":-330},"id":"KcfuxQ-T9hXR","outputId":"e84bb3ec-07ef-4f4d-bd5b-cbcc3b813ab2","scrolled":true},"outputs":[{"name":"stderr","output_type":"stream","text":["/usr/local/lib/python3.7/dist-packages/sklearn/linear_model/_logistic.py:818: ConvergenceWarning: lbfgs failed to converge (status=1):\n","STOP: TOTAL NO. of ITERATIONS REACHED LIMIT.\n","\n","Increase the number of iterations (max_iter) or scale the data as shown in:\n","    https://scikit-learn.org/stable/modules/preprocessing.html\n","Please also refer to the documentation for alternative solver options:\n","    https://scikit-learn.org/stable/modules/linear_model.html#logistic-regression\n","  extra_warning_msg=_LOGISTIC_SOLVER_CONVERGENCE_MSG,\n"]},{"name":"stdout","output_type":"stream","text":["toxic score: 0.7203\n","severe_toxic score: 0.3203\n","obscene score: 0.7464\n","threat score: 0.1982\n","insult score: 0.6261\n","identity_hate score: 0.2785\n","any_label score: 0.7295\n","Elapsed time was 0:55.\n"]},{"name":"stderr","output_type":"stream","text":["/usr/local/lib/python3.7/dist-packages/sklearn/linear_model/_logistic.py:818: ConvergenceWarning: lbfgs failed to converge (status=1):\n","STOP: TOTAL NO. of ITERATIONS REACHED LIMIT.\n","\n","Increase the number of iterations (max_iter) or scale the data as shown in:\n","    https://scikit-learn.org/stable/modules/preprocessing.html\n","Please also refer to the documentation for alternative solver options:\n","    https://scikit-learn.org/stable/modules/linear_model.html#logistic-regression\n","  extra_warning_msg=_LOGISTIC_SOLVER_CONVERGENCE_MSG,\n"]}],"source":["start = time.time()\n","for target in targets: \n","    lr = LogisticRegression(random_state=seed)\n","    print(target + ' score: %.4f' % np.mean(cross_val_score(lr, training_comments, df_targets[target], scoring='f1', cv=cv)))\n","print_time(start)"]},{"cell_type":"markdown","metadata":{"id":"q6dQLxTl9hXT"},"source":["With engineered features added in. "]},{"cell_type":"code","execution_count":null,"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"elapsed":2557,"status":"ok","timestamp":1664790302277,"user":{"displayName":"akhilvydyula01 dump","userId":"04060569174740840718"},"user_tz":-330},"id":"Q32IJojL9hXU","outputId":"05a52246-f2fe-422a-db81-0cf039e1d470"},"outputs":[{"name":"stderr","output_type":"stream","text":["/usr/local/lib/python3.7/dist-packages/sklearn/model_selection/_validation.py:372: FitFailedWarning: \n","5 fits failed out of a total of 5.\n","The score on these train-test partitions for these parameters will be set to nan.\n","If these failures are not expected, you can try to debug them by setting error_score='raise'.\n","\n","Below are more details about the failures:\n","--------------------------------------------------------------------------------\n","5 fits failed with the following error:\n","Traceback (most recent call last):\n","  File \"/usr/local/lib/python3.7/dist-packages/sklearn/model_selection/_validation.py\", line 680, in _fit_and_score\n","    estimator.fit(X_train, y_train, **fit_params)\n","  File \"/usr/local/lib/python3.7/dist-packages/sklearn/linear_model/_logistic.py\", line 1514, in fit\n","    accept_large_sparse=solver not in [\"liblinear\", \"sag\", \"saga\"],\n","  File \"/usr/local/lib/python3.7/dist-packages/sklearn/base.py\", line 581, in _validate_data\n","    X, y = check_X_y(X, y, **check_params)\n","  File \"/usr/local/lib/python3.7/dist-packages/sklearn/utils/validation.py\", line 976, in check_X_y\n","    estimator=estimator,\n","  File \"/usr/local/lib/python3.7/dist-packages/sklearn/utils/validation.py\", line 726, in check_array\n","    accept_large_sparse=accept_large_sparse,\n","  File \"/usr/local/lib/python3.7/dist-packages/sklearn/utils/validation.py\", line 479, in _ensure_sparse_format\n","    _assert_all_finite(spmatrix.data, allow_nan=force_all_finite == \"allow-nan\")\n","  File \"/usr/local/lib/python3.7/dist-packages/sklearn/utils/validation.py\", line 116, in _assert_all_finite\n","    type_err, msg_dtype if msg_dtype is not None else X.dtype\n","ValueError: Input contains NaN, infinity or a value too large for dtype('float64').\n","\n","  warnings.warn(some_fits_failed_message, FitFailedWarning)\n"]},{"name":"stdout","output_type":"stream","text":["toxic score: nan\n"]},{"name":"stderr","output_type":"stream","text":["/usr/local/lib/python3.7/dist-packages/sklearn/model_selection/_validation.py:372: FitFailedWarning: \n","5 fits failed out of a total of 5.\n","The score on these train-test partitions for these parameters will be set to nan.\n","If these failures are not expected, you can try to debug them by setting error_score='raise'.\n","\n","Below are more details about the failures:\n","--------------------------------------------------------------------------------\n","5 fits failed with the following error:\n","Traceback (most recent call last):\n","  File \"/usr/local/lib/python3.7/dist-packages/sklearn/model_selection/_validation.py\", line 680, in _fit_and_score\n","    estimator.fit(X_train, y_train, **fit_params)\n","  File \"/usr/local/lib/python3.7/dist-packages/sklearn/linear_model/_logistic.py\", line 1514, in fit\n","    accept_large_sparse=solver not in [\"liblinear\", \"sag\", \"saga\"],\n","  File \"/usr/local/lib/python3.7/dist-packages/sklearn/base.py\", line 581, in _validate_data\n","    X, y = check_X_y(X, y, **check_params)\n","  File \"/usr/local/lib/python3.7/dist-packages/sklearn/utils/validation.py\", line 976, in check_X_y\n","    estimator=estimator,\n","  File \"/usr/local/lib/python3.7/dist-packages/sklearn/utils/validation.py\", line 726, in check_array\n","    accept_large_sparse=accept_large_sparse,\n","  File \"/usr/local/lib/python3.7/dist-packages/sklearn/utils/validation.py\", line 479, in _ensure_sparse_format\n","    _assert_all_finite(spmatrix.data, allow_nan=force_all_finite == \"allow-nan\")\n","  File \"/usr/local/lib/python3.7/dist-packages/sklearn/utils/validation.py\", line 116, in _assert_all_finite\n","    type_err, msg_dtype if msg_dtype is not None else X.dtype\n","ValueError: Input contains NaN, infinity or a value too large for dtype('float64').\n","\n","  warnings.warn(some_fits_failed_message, FitFailedWarning)\n"]},{"name":"stdout","output_type":"stream","text":["severe_toxic score: nan\n"]},{"name":"stderr","output_type":"stream","text":["/usr/local/lib/python3.7/dist-packages/sklearn/model_selection/_validation.py:372: FitFailedWarning: \n","5 fits failed out of a total of 5.\n","The score on these train-test partitions for these parameters will be set to nan.\n","If these failures are not expected, you can try to debug them by setting error_score='raise'.\n","\n","Below are more details about the failures:\n","--------------------------------------------------------------------------------\n","5 fits failed with the following error:\n","Traceback (most recent call last):\n","  File \"/usr/local/lib/python3.7/dist-packages/sklearn/model_selection/_validation.py\", line 680, in _fit_and_score\n","    estimator.fit(X_train, y_train, **fit_params)\n","  File \"/usr/local/lib/python3.7/dist-packages/sklearn/linear_model/_logistic.py\", line 1514, in fit\n","    accept_large_sparse=solver not in [\"liblinear\", \"sag\", \"saga\"],\n","  File \"/usr/local/lib/python3.7/dist-packages/sklearn/base.py\", line 581, in _validate_data\n","    X, y = check_X_y(X, y, **check_params)\n","  File \"/usr/local/lib/python3.7/dist-packages/sklearn/utils/validation.py\", line 976, in check_X_y\n","    estimator=estimator,\n","  File \"/usr/local/lib/python3.7/dist-packages/sklearn/utils/validation.py\", line 726, in check_array\n","    accept_large_sparse=accept_large_sparse,\n","  File \"/usr/local/lib/python3.7/dist-packages/sklearn/utils/validation.py\", line 479, in _ensure_sparse_format\n","    _assert_all_finite(spmatrix.data, allow_nan=force_all_finite == \"allow-nan\")\n","  File \"/usr/local/lib/python3.7/dist-packages/sklearn/utils/validation.py\", line 116, in _assert_all_finite\n","    type_err, msg_dtype if msg_dtype is not None else X.dtype\n","ValueError: Input contains NaN, infinity or a value too large for dtype('float64').\n","\n","  warnings.warn(some_fits_failed_message, FitFailedWarning)\n"]},{"name":"stdout","output_type":"stream","text":["obscene score: nan\n"]},{"name":"stderr","output_type":"stream","text":["/usr/local/lib/python3.7/dist-packages/sklearn/model_selection/_validation.py:372: FitFailedWarning: \n","5 fits failed out of a total of 5.\n","The score on these train-test partitions for these parameters will be set to nan.\n","If these failures are not expected, you can try to debug them by setting error_score='raise'.\n","\n","Below are more details about the failures:\n","--------------------------------------------------------------------------------\n","5 fits failed with the following error:\n","Traceback (most recent call last):\n","  File \"/usr/local/lib/python3.7/dist-packages/sklearn/model_selection/_validation.py\", line 680, in _fit_and_score\n","    estimator.fit(X_train, y_train, **fit_params)\n","  File \"/usr/local/lib/python3.7/dist-packages/sklearn/linear_model/_logistic.py\", line 1514, in fit\n","    accept_large_sparse=solver not in [\"liblinear\", \"sag\", \"saga\"],\n","  File \"/usr/local/lib/python3.7/dist-packages/sklearn/base.py\", line 581, in _validate_data\n","    X, y = check_X_y(X, y, **check_params)\n","  File \"/usr/local/lib/python3.7/dist-packages/sklearn/utils/validation.py\", line 976, in check_X_y\n","    estimator=estimator,\n","  File \"/usr/local/lib/python3.7/dist-packages/sklearn/utils/validation.py\", line 726, in check_array\n","    accept_large_sparse=accept_large_sparse,\n","  File \"/usr/local/lib/python3.7/dist-packages/sklearn/utils/validation.py\", line 479, in _ensure_sparse_format\n","    _assert_all_finite(spmatrix.data, allow_nan=force_all_finite == \"allow-nan\")\n","  File \"/usr/local/lib/python3.7/dist-packages/sklearn/utils/validation.py\", line 116, in _assert_all_finite\n","    type_err, msg_dtype if msg_dtype is not None else X.dtype\n","ValueError: Input contains NaN, infinity or a value too large for dtype('float64').\n","\n","  warnings.warn(some_fits_failed_message, FitFailedWarning)\n"]},{"name":"stdout","output_type":"stream","text":["threat score: nan\n"]},{"name":"stderr","output_type":"stream","text":["/usr/local/lib/python3.7/dist-packages/sklearn/model_selection/_validation.py:372: FitFailedWarning: \n","5 fits failed out of a total of 5.\n","The score on these train-test partitions for these parameters will be set to nan.\n","If these failures are not expected, you can try to debug them by setting error_score='raise'.\n","\n","Below are more details about the failures:\n","--------------------------------------------------------------------------------\n","5 fits failed with the following error:\n","Traceback (most recent call last):\n","  File \"/usr/local/lib/python3.7/dist-packages/sklearn/model_selection/_validation.py\", line 680, in _fit_and_score\n","    estimator.fit(X_train, y_train, **fit_params)\n","  File \"/usr/local/lib/python3.7/dist-packages/sklearn/linear_model/_logistic.py\", line 1514, in fit\n","    accept_large_sparse=solver not in [\"liblinear\", \"sag\", \"saga\"],\n","  File \"/usr/local/lib/python3.7/dist-packages/sklearn/base.py\", line 581, in _validate_data\n","    X, y = check_X_y(X, y, **check_params)\n","  File \"/usr/local/lib/python3.7/dist-packages/sklearn/utils/validation.py\", line 976, in check_X_y\n","    estimator=estimator,\n","  File \"/usr/local/lib/python3.7/dist-packages/sklearn/utils/validation.py\", line 726, in check_array\n","    accept_large_sparse=accept_large_sparse,\n","  File \"/usr/local/lib/python3.7/dist-packages/sklearn/utils/validation.py\", line 479, in _ensure_sparse_format\n","    _assert_all_finite(spmatrix.data, allow_nan=force_all_finite == \"allow-nan\")\n","  File \"/usr/local/lib/python3.7/dist-packages/sklearn/utils/validation.py\", line 116, in _assert_all_finite\n","    type_err, msg_dtype if msg_dtype is not None else X.dtype\n","ValueError: Input contains NaN, infinity or a value too large for dtype('float64').\n","\n","  warnings.warn(some_fits_failed_message, FitFailedWarning)\n"]},{"name":"stdout","output_type":"stream","text":["insult score: nan\n"]},{"name":"stderr","output_type":"stream","text":["/usr/local/lib/python3.7/dist-packages/sklearn/model_selection/_validation.py:372: FitFailedWarning: \n","5 fits failed out of a total of 5.\n","The score on these train-test partitions for these parameters will be set to nan.\n","If these failures are not expected, you can try to debug them by setting error_score='raise'.\n","\n","Below are more details about the failures:\n","--------------------------------------------------------------------------------\n","5 fits failed with the following error:\n","Traceback (most recent call last):\n","  File \"/usr/local/lib/python3.7/dist-packages/sklearn/model_selection/_validation.py\", line 680, in _fit_and_score\n","    estimator.fit(X_train, y_train, **fit_params)\n","  File \"/usr/local/lib/python3.7/dist-packages/sklearn/linear_model/_logistic.py\", line 1514, in fit\n","    accept_large_sparse=solver not in [\"liblinear\", \"sag\", \"saga\"],\n","  File \"/usr/local/lib/python3.7/dist-packages/sklearn/base.py\", line 581, in _validate_data\n","    X, y = check_X_y(X, y, **check_params)\n","  File \"/usr/local/lib/python3.7/dist-packages/sklearn/utils/validation.py\", line 976, in check_X_y\n","    estimator=estimator,\n","  File \"/usr/local/lib/python3.7/dist-packages/sklearn/utils/validation.py\", line 726, in check_array\n","    accept_large_sparse=accept_large_sparse,\n","  File \"/usr/local/lib/python3.7/dist-packages/sklearn/utils/validation.py\", line 479, in _ensure_sparse_format\n","    _assert_all_finite(spmatrix.data, allow_nan=force_all_finite == \"allow-nan\")\n","  File \"/usr/local/lib/python3.7/dist-packages/sklearn/utils/validation.py\", line 116, in _assert_all_finite\n","    type_err, msg_dtype if msg_dtype is not None else X.dtype\n","ValueError: Input contains NaN, infinity or a value too large for dtype('float64').\n","\n","  warnings.warn(some_fits_failed_message, FitFailedWarning)\n"]},{"name":"stdout","output_type":"stream","text":["identity_hate score: nan\n","any_label score: nan\n","Elapsed time was 0:02.\n"]},{"name":"stderr","output_type":"stream","text":["/usr/local/lib/python3.7/dist-packages/sklearn/model_selection/_validation.py:372: FitFailedWarning: \n","5 fits failed out of a total of 5.\n","The score on these train-test partitions for these parameters will be set to nan.\n","If these failures are not expected, you can try to debug them by setting error_score='raise'.\n","\n","Below are more details about the failures:\n","--------------------------------------------------------------------------------\n","5 fits failed with the following error:\n","Traceback (most recent call last):\n","  File \"/usr/local/lib/python3.7/dist-packages/sklearn/model_selection/_validation.py\", line 680, in _fit_and_score\n","    estimator.fit(X_train, y_train, **fit_params)\n","  File \"/usr/local/lib/python3.7/dist-packages/sklearn/linear_model/_logistic.py\", line 1514, in fit\n","    accept_large_sparse=solver not in [\"liblinear\", \"sag\", \"saga\"],\n","  File \"/usr/local/lib/python3.7/dist-packages/sklearn/base.py\", line 581, in _validate_data\n","    X, y = check_X_y(X, y, **check_params)\n","  File \"/usr/local/lib/python3.7/dist-packages/sklearn/utils/validation.py\", line 976, in check_X_y\n","    estimator=estimator,\n","  File \"/usr/local/lib/python3.7/dist-packages/sklearn/utils/validation.py\", line 726, in check_array\n","    accept_large_sparse=accept_large_sparse,\n","  File \"/usr/local/lib/python3.7/dist-packages/sklearn/utils/validation.py\", line 479, in _ensure_sparse_format\n","    _assert_all_finite(spmatrix.data, allow_nan=force_all_finite == \"allow-nan\")\n","  File \"/usr/local/lib/python3.7/dist-packages/sklearn/utils/validation.py\", line 116, in _assert_all_finite\n","    type_err, msg_dtype if msg_dtype is not None else X.dtype\n","ValueError: Input contains NaN, infinity or a value too large for dtype('float64').\n","\n","  warnings.warn(some_fits_failed_message, FitFailedWarning)\n"]}],"source":["start = time.time()\n","for target in targets: \n","    lr = LogisticRegression(random_state=seed)\n","    print(target + ' score: %.4f' % np.mean(cross_val_score(lr, merge_features(training_comments, df, new_features), df_targets[target], scoring='f1', cv=cv)))\n","print_time(start)"]},{"cell_type":"markdown","metadata":{"id":"c4vgs3Zg9hXV"},"source":["### Naive Bayes"]},{"cell_type":"code","execution_count":null,"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"elapsed":1604,"status":"ok","timestamp":1664790303847,"user":{"displayName":"akhilvydyula01 dump","userId":"04060569174740840718"},"user_tz":-330},"id":"pvxOwoGi9hXX","outputId":"648c175d-193c-47bd-f686-d59808d9b448"},"outputs":[{"name":"stdout","output_type":"stream","text":["toxic f1 score: 0.6581\n","severe_toxic f1 score: 0.1044\n","obscene f1 score: 0.6668\n","threat f1 score: 0.0000\n","insult f1 score: 0.5604\n","identity_hate f1 score: 0.0418\n","any_label f1 score: 0.6670\n","Average (excluding any) f1 score: 0.3386\n","Elapsed time was 0:02.\n"]}],"source":["start = time.time() \n","\n","model = MultinomialNB(alpha=1.0)\n","_ = multi_cv(model, training_comments, df_targets)\n","# _ = multi_cv(model, training_comments, df_targets,)\n","print_time(start)"]},{"cell_type":"markdown","metadata":{"id":"4vy7vov89hXY"},"source":["With engineered features. "]},{"cell_type":"code","execution_count":null,"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"elapsed":1466,"status":"ok","timestamp":1664790305302,"user":{"displayName":"akhilvydyula01 dump","userId":"04060569174740840718"},"user_tz":-330},"id":"V7CyVwGQ9hXZ","outputId":"0722ea19-5522-42ab-e85d-fa7035d8bad0"},"outputs":[{"name":"stderr","output_type":"stream","text":["/usr/local/lib/python3.7/dist-packages/sklearn/model_selection/_validation.py:372: FitFailedWarning: \n","5 fits failed out of a total of 5.\n","The score on these train-test partitions for these parameters will be set to nan.\n","If these failures are not expected, you can try to debug them by setting error_score='raise'.\n","\n","Below are more details about the failures:\n","--------------------------------------------------------------------------------\n","5 fits failed with the following error:\n","Traceback (most recent call last):\n","  File \"/usr/local/lib/python3.7/dist-packages/sklearn/model_selection/_validation.py\", line 680, in _fit_and_score\n","    estimator.fit(X_train, y_train, **fit_params)\n","  File \"/usr/local/lib/python3.7/dist-packages/sklearn/naive_bayes.py\", line 663, in fit\n","    X, y = self._check_X_y(X, y)\n","  File \"/usr/local/lib/python3.7/dist-packages/sklearn/naive_bayes.py\", line 523, in _check_X_y\n","    return self._validate_data(X, y, accept_sparse=\"csr\", reset=reset)\n","  File \"/usr/local/lib/python3.7/dist-packages/sklearn/base.py\", line 581, in _validate_data\n","    X, y = check_X_y(X, y, **check_params)\n","  File \"/usr/local/lib/python3.7/dist-packages/sklearn/utils/validation.py\", line 976, in check_X_y\n","    estimator=estimator,\n","  File \"/usr/local/lib/python3.7/dist-packages/sklearn/utils/validation.py\", line 726, in check_array\n","    accept_large_sparse=accept_large_sparse,\n","  File \"/usr/local/lib/python3.7/dist-packages/sklearn/utils/validation.py\", line 479, in _ensure_sparse_format\n","    _assert_all_finite(spmatrix.data, allow_nan=force_all_finite == \"allow-nan\")\n","  File \"/usr/local/lib/python3.7/dist-packages/sklearn/utils/validation.py\", line 116, in _assert_all_finite\n","    type_err, msg_dtype if msg_dtype is not None else X.dtype\n","ValueError: Input contains NaN, infinity or a value too large for dtype('float64').\n","\n","  warnings.warn(some_fits_failed_message, FitFailedWarning)\n","/usr/local/lib/python3.7/dist-packages/sklearn/model_selection/_validation.py:372: FitFailedWarning: \n","5 fits failed out of a total of 5.\n","The score on these train-test partitions for these parameters will be set to nan.\n","If these failures are not expected, you can try to debug them by setting error_score='raise'.\n","\n","Below are more details about the failures:\n","--------------------------------------------------------------------------------\n","5 fits failed with the following error:\n","Traceback (most recent call last):\n","  File \"/usr/local/lib/python3.7/dist-packages/sklearn/model_selection/_validation.py\", line 680, in _fit_and_score\n","    estimator.fit(X_train, y_train, **fit_params)\n","  File \"/usr/local/lib/python3.7/dist-packages/sklearn/naive_bayes.py\", line 663, in fit\n","    X, y = self._check_X_y(X, y)\n","  File \"/usr/local/lib/python3.7/dist-packages/sklearn/naive_bayes.py\", line 523, in _check_X_y\n","    return self._validate_data(X, y, accept_sparse=\"csr\", reset=reset)\n","  File \"/usr/local/lib/python3.7/dist-packages/sklearn/base.py\", line 581, in _validate_data\n","    X, y = check_X_y(X, y, **check_params)\n","  File \"/usr/local/lib/python3.7/dist-packages/sklearn/utils/validation.py\", line 976, in check_X_y\n","    estimator=estimator,\n","  File \"/usr/local/lib/python3.7/dist-packages/sklearn/utils/validation.py\", line 726, in check_array\n","    accept_large_sparse=accept_large_sparse,\n","  File \"/usr/local/lib/python3.7/dist-packages/sklearn/utils/validation.py\", line 479, in _ensure_sparse_format\n","    _assert_all_finite(spmatrix.data, allow_nan=force_all_finite == \"allow-nan\")\n","  File \"/usr/local/lib/python3.7/dist-packages/sklearn/utils/validation.py\", line 116, in _assert_all_finite\n","    type_err, msg_dtype if msg_dtype is not None else X.dtype\n","ValueError: Input contains NaN, infinity or a value too large for dtype('float64').\n","\n","  warnings.warn(some_fits_failed_message, FitFailedWarning)\n"]},{"name":"stdout","output_type":"stream","text":["toxic f1 score: nan\n","severe_toxic f1 score: nan\n"]},{"name":"stderr","output_type":"stream","text":["/usr/local/lib/python3.7/dist-packages/sklearn/model_selection/_validation.py:372: FitFailedWarning: \n","5 fits failed out of a total of 5.\n","The score on these train-test partitions for these parameters will be set to nan.\n","If these failures are not expected, you can try to debug them by setting error_score='raise'.\n","\n","Below are more details about the failures:\n","--------------------------------------------------------------------------------\n","5 fits failed with the following error:\n","Traceback (most recent call last):\n","  File \"/usr/local/lib/python3.7/dist-packages/sklearn/model_selection/_validation.py\", line 680, in _fit_and_score\n","    estimator.fit(X_train, y_train, **fit_params)\n","  File \"/usr/local/lib/python3.7/dist-packages/sklearn/naive_bayes.py\", line 663, in fit\n","    X, y = self._check_X_y(X, y)\n","  File \"/usr/local/lib/python3.7/dist-packages/sklearn/naive_bayes.py\", line 523, in _check_X_y\n","    return self._validate_data(X, y, accept_sparse=\"csr\", reset=reset)\n","  File \"/usr/local/lib/python3.7/dist-packages/sklearn/base.py\", line 581, in _validate_data\n","    X, y = check_X_y(X, y, **check_params)\n","  File \"/usr/local/lib/python3.7/dist-packages/sklearn/utils/validation.py\", line 976, in check_X_y\n","    estimator=estimator,\n","  File \"/usr/local/lib/python3.7/dist-packages/sklearn/utils/validation.py\", line 726, in check_array\n","    accept_large_sparse=accept_large_sparse,\n","  File \"/usr/local/lib/python3.7/dist-packages/sklearn/utils/validation.py\", line 479, in _ensure_sparse_format\n","    _assert_all_finite(spmatrix.data, allow_nan=force_all_finite == \"allow-nan\")\n","  File \"/usr/local/lib/python3.7/dist-packages/sklearn/utils/validation.py\", line 116, in _assert_all_finite\n","    type_err, msg_dtype if msg_dtype is not None else X.dtype\n","ValueError: Input contains NaN, infinity or a value too large for dtype('float64').\n","\n","  warnings.warn(some_fits_failed_message, FitFailedWarning)\n"]},{"name":"stdout","output_type":"stream","text":["obscene f1 score: nan\n"]},{"name":"stderr","output_type":"stream","text":["/usr/local/lib/python3.7/dist-packages/sklearn/model_selection/_validation.py:372: FitFailedWarning: \n","5 fits failed out of a total of 5.\n","The score on these train-test partitions for these parameters will be set to nan.\n","If these failures are not expected, you can try to debug them by setting error_score='raise'.\n","\n","Below are more details about the failures:\n","--------------------------------------------------------------------------------\n","5 fits failed with the following error:\n","Traceback (most recent call last):\n","  File \"/usr/local/lib/python3.7/dist-packages/sklearn/model_selection/_validation.py\", line 680, in _fit_and_score\n","    estimator.fit(X_train, y_train, **fit_params)\n","  File \"/usr/local/lib/python3.7/dist-packages/sklearn/naive_bayes.py\", line 663, in fit\n","    X, y = self._check_X_y(X, y)\n","  File \"/usr/local/lib/python3.7/dist-packages/sklearn/naive_bayes.py\", line 523, in _check_X_y\n","    return self._validate_data(X, y, accept_sparse=\"csr\", reset=reset)\n","  File \"/usr/local/lib/python3.7/dist-packages/sklearn/base.py\", line 581, in _validate_data\n","    X, y = check_X_y(X, y, **check_params)\n","  File \"/usr/local/lib/python3.7/dist-packages/sklearn/utils/validation.py\", line 976, in check_X_y\n","    estimator=estimator,\n","  File \"/usr/local/lib/python3.7/dist-packages/sklearn/utils/validation.py\", line 726, in check_array\n","    accept_large_sparse=accept_large_sparse,\n","  File \"/usr/local/lib/python3.7/dist-packages/sklearn/utils/validation.py\", line 479, in _ensure_sparse_format\n","    _assert_all_finite(spmatrix.data, allow_nan=force_all_finite == \"allow-nan\")\n","  File \"/usr/local/lib/python3.7/dist-packages/sklearn/utils/validation.py\", line 116, in _assert_all_finite\n","    type_err, msg_dtype if msg_dtype is not None else X.dtype\n","ValueError: Input contains NaN, infinity or a value too large for dtype('float64').\n","\n","  warnings.warn(some_fits_failed_message, FitFailedWarning)\n"]},{"name":"stdout","output_type":"stream","text":["threat f1 score: nan\n"]},{"name":"stderr","output_type":"stream","text":["/usr/local/lib/python3.7/dist-packages/sklearn/model_selection/_validation.py:372: FitFailedWarning: \n","5 fits failed out of a total of 5.\n","The score on these train-test partitions for these parameters will be set to nan.\n","If these failures are not expected, you can try to debug them by setting error_score='raise'.\n","\n","Below are more details about the failures:\n","--------------------------------------------------------------------------------\n","5 fits failed with the following error:\n","Traceback (most recent call last):\n","  File \"/usr/local/lib/python3.7/dist-packages/sklearn/model_selection/_validation.py\", line 680, in _fit_and_score\n","    estimator.fit(X_train, y_train, **fit_params)\n","  File \"/usr/local/lib/python3.7/dist-packages/sklearn/naive_bayes.py\", line 663, in fit\n","    X, y = self._check_X_y(X, y)\n","  File \"/usr/local/lib/python3.7/dist-packages/sklearn/naive_bayes.py\", line 523, in _check_X_y\n","    return self._validate_data(X, y, accept_sparse=\"csr\", reset=reset)\n","  File \"/usr/local/lib/python3.7/dist-packages/sklearn/base.py\", line 581, in _validate_data\n","    X, y = check_X_y(X, y, **check_params)\n","  File \"/usr/local/lib/python3.7/dist-packages/sklearn/utils/validation.py\", line 976, in check_X_y\n","    estimator=estimator,\n","  File \"/usr/local/lib/python3.7/dist-packages/sklearn/utils/validation.py\", line 726, in check_array\n","    accept_large_sparse=accept_large_sparse,\n","  File \"/usr/local/lib/python3.7/dist-packages/sklearn/utils/validation.py\", line 479, in _ensure_sparse_format\n","    _assert_all_finite(spmatrix.data, allow_nan=force_all_finite == \"allow-nan\")\n","  File \"/usr/local/lib/python3.7/dist-packages/sklearn/utils/validation.py\", line 116, in _assert_all_finite\n","    type_err, msg_dtype if msg_dtype is not None else X.dtype\n","ValueError: Input contains NaN, infinity or a value too large for dtype('float64').\n","\n","  warnings.warn(some_fits_failed_message, FitFailedWarning)\n"]},{"name":"stdout","output_type":"stream","text":["insult f1 score: nan\n"]},{"name":"stderr","output_type":"stream","text":["/usr/local/lib/python3.7/dist-packages/sklearn/model_selection/_validation.py:372: FitFailedWarning: \n","5 fits failed out of a total of 5.\n","The score on these train-test partitions for these parameters will be set to nan.\n","If these failures are not expected, you can try to debug them by setting error_score='raise'.\n","\n","Below are more details about the failures:\n","--------------------------------------------------------------------------------\n","5 fits failed with the following error:\n","Traceback (most recent call last):\n","  File \"/usr/local/lib/python3.7/dist-packages/sklearn/model_selection/_validation.py\", line 680, in _fit_and_score\n","    estimator.fit(X_train, y_train, **fit_params)\n","  File \"/usr/local/lib/python3.7/dist-packages/sklearn/naive_bayes.py\", line 663, in fit\n","    X, y = self._check_X_y(X, y)\n","  File \"/usr/local/lib/python3.7/dist-packages/sklearn/naive_bayes.py\", line 523, in _check_X_y\n","    return self._validate_data(X, y, accept_sparse=\"csr\", reset=reset)\n","  File \"/usr/local/lib/python3.7/dist-packages/sklearn/base.py\", line 581, in _validate_data\n","    X, y = check_X_y(X, y, **check_params)\n","  File \"/usr/local/lib/python3.7/dist-packages/sklearn/utils/validation.py\", line 976, in check_X_y\n","    estimator=estimator,\n","  File \"/usr/local/lib/python3.7/dist-packages/sklearn/utils/validation.py\", line 726, in check_array\n","    accept_large_sparse=accept_large_sparse,\n","  File \"/usr/local/lib/python3.7/dist-packages/sklearn/utils/validation.py\", line 479, in _ensure_sparse_format\n","    _assert_all_finite(spmatrix.data, allow_nan=force_all_finite == \"allow-nan\")\n","  File \"/usr/local/lib/python3.7/dist-packages/sklearn/utils/validation.py\", line 116, in _assert_all_finite\n","    type_err, msg_dtype if msg_dtype is not None else X.dtype\n","ValueError: Input contains NaN, infinity or a value too large for dtype('float64').\n","\n","  warnings.warn(some_fits_failed_message, FitFailedWarning)\n"]},{"name":"stdout","output_type":"stream","text":["identity_hate f1 score: nan\n","any_label f1 score: nan\n","Average (excluding any) f1 score: nan\n","Elapsed time was 0:01.\n"]},{"name":"stderr","output_type":"stream","text":["/usr/local/lib/python3.7/dist-packages/sklearn/model_selection/_validation.py:372: FitFailedWarning: \n","5 fits failed out of a total of 5.\n","The score on these train-test partitions for these parameters will be set to nan.\n","If these failures are not expected, you can try to debug them by setting error_score='raise'.\n","\n","Below are more details about the failures:\n","--------------------------------------------------------------------------------\n","5 fits failed with the following error:\n","Traceback (most recent call last):\n","  File \"/usr/local/lib/python3.7/dist-packages/sklearn/model_selection/_validation.py\", line 680, in _fit_and_score\n","    estimator.fit(X_train, y_train, **fit_params)\n","  File \"/usr/local/lib/python3.7/dist-packages/sklearn/naive_bayes.py\", line 663, in fit\n","    X, y = self._check_X_y(X, y)\n","  File \"/usr/local/lib/python3.7/dist-packages/sklearn/naive_bayes.py\", line 523, in _check_X_y\n","    return self._validate_data(X, y, accept_sparse=\"csr\", reset=reset)\n","  File \"/usr/local/lib/python3.7/dist-packages/sklearn/base.py\", line 581, in _validate_data\n","    X, y = check_X_y(X, y, **check_params)\n","  File \"/usr/local/lib/python3.7/dist-packages/sklearn/utils/validation.py\", line 976, in check_X_y\n","    estimator=estimator,\n","  File \"/usr/local/lib/python3.7/dist-packages/sklearn/utils/validation.py\", line 726, in check_array\n","    accept_large_sparse=accept_large_sparse,\n","  File \"/usr/local/lib/python3.7/dist-packages/sklearn/utils/validation.py\", line 479, in _ensure_sparse_format\n","    _assert_all_finite(spmatrix.data, allow_nan=force_all_finite == \"allow-nan\")\n","  File \"/usr/local/lib/python3.7/dist-packages/sklearn/utils/validation.py\", line 116, in _assert_all_finite\n","    type_err, msg_dtype if msg_dtype is not None else X.dtype\n","ValueError: Input contains NaN, infinity or a value too large for dtype('float64').\n","\n","  warnings.warn(some_fits_failed_message, FitFailedWarning)\n"]}],"source":["start = time.time() \n","\n","model = MultinomialNB(alpha=1.0)\n","_ = multi_cv(model, merge_features(training_comments, df, new_features), df_targets)\n","print_time(start)"]},{"cell_type":"markdown","metadata":{"id":"08hxdepG9hXa"},"source":["### Support Vector Machine "]},{"cell_type":"code","execution_count":null,"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"elapsed":10926,"status":"ok","timestamp":1664790316207,"user":{"displayName":"akhilvydyula01 dump","userId":"04060569174740840718"},"user_tz":-330},"id":"bD_sTjiY9hXb","outputId":"4ab33beb-b3a0-4ea7-ffb3-7179a7141557"},"outputs":[{"name":"stdout","output_type":"stream","text":["toxic f1 score: 0.7549\n","severe_toxic f1 score: 0.3402\n","obscene f1 score: 0.7809\n","threat f1 score: 0.3648\n","insult f1 score: 0.6645\n","identity_hate f1 score: 0.3562\n","any_label f1 score: 0.7703\n","Average (excluding any) f1 score: 0.5436\n","Elapsed time was 0:10.\n"]}],"source":["start = time.time()\n","model = LinearSVC(random_state=None)\n","_ = multi_cv(model, training_comments, df_targets)\n","print_time(start)"]},{"cell_type":"code","execution_count":null,"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"elapsed":877,"status":"ok","timestamp":1664790317010,"user":{"displayName":"akhilvydyula01 dump","userId":"04060569174740840718"},"user_tz":-330},"id":"z09x1QVg9hXc","outputId":"6c387472-815d-4ae6-dc55-f7a43c1e892a"},"outputs":[{"name":"stderr","output_type":"stream","text":["/usr/local/lib/python3.7/dist-packages/sklearn/model_selection/_validation.py:372: FitFailedWarning: \n","5 fits failed out of a total of 5.\n","The score on these train-test partitions for these parameters will be set to nan.\n","If these failures are not expected, you can try to debug them by setting error_score='raise'.\n","\n","Below are more details about the failures:\n","--------------------------------------------------------------------------------\n","5 fits failed with the following error:\n","Traceback (most recent call last):\n","  File \"/usr/local/lib/python3.7/dist-packages/sklearn/model_selection/_validation.py\", line 680, in _fit_and_score\n","    estimator.fit(X_train, y_train, **fit_params)\n","  File \"/usr/local/lib/python3.7/dist-packages/sklearn/svm/_classes.py\", line 252, in fit\n","    accept_large_sparse=False,\n","  File \"/usr/local/lib/python3.7/dist-packages/sklearn/base.py\", line 581, in _validate_data\n","    X, y = check_X_y(X, y, **check_params)\n","  File \"/usr/local/lib/python3.7/dist-packages/sklearn/utils/validation.py\", line 976, in check_X_y\n","    estimator=estimator,\n","  File \"/usr/local/lib/python3.7/dist-packages/sklearn/utils/validation.py\", line 726, in check_array\n","    accept_large_sparse=accept_large_sparse,\n","  File \"/usr/local/lib/python3.7/dist-packages/sklearn/utils/validation.py\", line 479, in _ensure_sparse_format\n","    _assert_all_finite(spmatrix.data, allow_nan=force_all_finite == \"allow-nan\")\n","  File \"/usr/local/lib/python3.7/dist-packages/sklearn/utils/validation.py\", line 116, in _assert_all_finite\n","    type_err, msg_dtype if msg_dtype is not None else X.dtype\n","ValueError: Input contains NaN, infinity or a value too large for dtype('float64').\n","\n","  warnings.warn(some_fits_failed_message, FitFailedWarning)\n"]},{"name":"stdout","output_type":"stream","text":["toxic f1 score: nan\n"]},{"name":"stderr","output_type":"stream","text":["/usr/local/lib/python3.7/dist-packages/sklearn/model_selection/_validation.py:372: FitFailedWarning: \n","5 fits failed out of a total of 5.\n","The score on these train-test partitions for these parameters will be set to nan.\n","If these failures are not expected, you can try to debug them by setting error_score='raise'.\n","\n","Below are more details about the failures:\n","--------------------------------------------------------------------------------\n","5 fits failed with the following error:\n","Traceback (most recent call last):\n","  File \"/usr/local/lib/python3.7/dist-packages/sklearn/model_selection/_validation.py\", line 680, in _fit_and_score\n","    estimator.fit(X_train, y_train, **fit_params)\n","  File \"/usr/local/lib/python3.7/dist-packages/sklearn/svm/_classes.py\", line 252, in fit\n","    accept_large_sparse=False,\n","  File \"/usr/local/lib/python3.7/dist-packages/sklearn/base.py\", line 581, in _validate_data\n","    X, y = check_X_y(X, y, **check_params)\n","  File \"/usr/local/lib/python3.7/dist-packages/sklearn/utils/validation.py\", line 976, in check_X_y\n","    estimator=estimator,\n","  File \"/usr/local/lib/python3.7/dist-packages/sklearn/utils/validation.py\", line 726, in check_array\n","    accept_large_sparse=accept_large_sparse,\n","  File \"/usr/local/lib/python3.7/dist-packages/sklearn/utils/validation.py\", line 479, in _ensure_sparse_format\n","    _assert_all_finite(spmatrix.data, allow_nan=force_all_finite == \"allow-nan\")\n","  File \"/usr/local/lib/python3.7/dist-packages/sklearn/utils/validation.py\", line 116, in _assert_all_finite\n","    type_err, msg_dtype if msg_dtype is not None else X.dtype\n","ValueError: Input contains NaN, infinity or a value too large for dtype('float64').\n","\n","  warnings.warn(some_fits_failed_message, FitFailedWarning)\n"]},{"name":"stdout","output_type":"stream","text":["severe_toxic f1 score: nan\n"]},{"name":"stderr","output_type":"stream","text":["/usr/local/lib/python3.7/dist-packages/sklearn/model_selection/_validation.py:372: FitFailedWarning: \n","5 fits failed out of a total of 5.\n","The score on these train-test partitions for these parameters will be set to nan.\n","If these failures are not expected, you can try to debug them by setting error_score='raise'.\n","\n","Below are more details about the failures:\n","--------------------------------------------------------------------------------\n","5 fits failed with the following error:\n","Traceback (most recent call last):\n","  File \"/usr/local/lib/python3.7/dist-packages/sklearn/model_selection/_validation.py\", line 680, in _fit_and_score\n","    estimator.fit(X_train, y_train, **fit_params)\n","  File \"/usr/local/lib/python3.7/dist-packages/sklearn/svm/_classes.py\", line 252, in fit\n","    accept_large_sparse=False,\n","  File \"/usr/local/lib/python3.7/dist-packages/sklearn/base.py\", line 581, in _validate_data\n","    X, y = check_X_y(X, y, **check_params)\n","  File \"/usr/local/lib/python3.7/dist-packages/sklearn/utils/validation.py\", line 976, in check_X_y\n","    estimator=estimator,\n","  File \"/usr/local/lib/python3.7/dist-packages/sklearn/utils/validation.py\", line 726, in check_array\n","    accept_large_sparse=accept_large_sparse,\n","  File \"/usr/local/lib/python3.7/dist-packages/sklearn/utils/validation.py\", line 479, in _ensure_sparse_format\n","    _assert_all_finite(spmatrix.data, allow_nan=force_all_finite == \"allow-nan\")\n","  File \"/usr/local/lib/python3.7/dist-packages/sklearn/utils/validation.py\", line 116, in _assert_all_finite\n","    type_err, msg_dtype if msg_dtype is not None else X.dtype\n","ValueError: Input contains NaN, infinity or a value too large for dtype('float64').\n","\n","  warnings.warn(some_fits_failed_message, FitFailedWarning)\n"]},{"name":"stdout","output_type":"stream","text":["obscene f1 score: nan\n"]},{"name":"stderr","output_type":"stream","text":["/usr/local/lib/python3.7/dist-packages/sklearn/model_selection/_validation.py:372: FitFailedWarning: \n","5 fits failed out of a total of 5.\n","The score on these train-test partitions for these parameters will be set to nan.\n","If these failures are not expected, you can try to debug them by setting error_score='raise'.\n","\n","Below are more details about the failures:\n","--------------------------------------------------------------------------------\n","5 fits failed with the following error:\n","Traceback (most recent call last):\n","  File \"/usr/local/lib/python3.7/dist-packages/sklearn/model_selection/_validation.py\", line 680, in _fit_and_score\n","    estimator.fit(X_train, y_train, **fit_params)\n","  File \"/usr/local/lib/python3.7/dist-packages/sklearn/svm/_classes.py\", line 252, in fit\n","    accept_large_sparse=False,\n","  File \"/usr/local/lib/python3.7/dist-packages/sklearn/base.py\", line 581, in _validate_data\n","    X, y = check_X_y(X, y, **check_params)\n","  File \"/usr/local/lib/python3.7/dist-packages/sklearn/utils/validation.py\", line 976, in check_X_y\n","    estimator=estimator,\n","  File \"/usr/local/lib/python3.7/dist-packages/sklearn/utils/validation.py\", line 726, in check_array\n","    accept_large_sparse=accept_large_sparse,\n","  File \"/usr/local/lib/python3.7/dist-packages/sklearn/utils/validation.py\", line 479, in _ensure_sparse_format\n","    _assert_all_finite(spmatrix.data, allow_nan=force_all_finite == \"allow-nan\")\n","  File \"/usr/local/lib/python3.7/dist-packages/sklearn/utils/validation.py\", line 116, in _assert_all_finite\n","    type_err, msg_dtype if msg_dtype is not None else X.dtype\n","ValueError: Input contains NaN, infinity or a value too large for dtype('float64').\n","\n","  warnings.warn(some_fits_failed_message, FitFailedWarning)\n"]},{"name":"stdout","output_type":"stream","text":["threat f1 score: nan\n"]},{"name":"stderr","output_type":"stream","text":["/usr/local/lib/python3.7/dist-packages/sklearn/model_selection/_validation.py:372: FitFailedWarning: \n","5 fits failed out of a total of 5.\n","The score on these train-test partitions for these parameters will be set to nan.\n","If these failures are not expected, you can try to debug them by setting error_score='raise'.\n","\n","Below are more details about the failures:\n","--------------------------------------------------------------------------------\n","5 fits failed with the following error:\n","Traceback (most recent call last):\n","  File \"/usr/local/lib/python3.7/dist-packages/sklearn/model_selection/_validation.py\", line 680, in _fit_and_score\n","    estimator.fit(X_train, y_train, **fit_params)\n","  File \"/usr/local/lib/python3.7/dist-packages/sklearn/svm/_classes.py\", line 252, in fit\n","    accept_large_sparse=False,\n","  File \"/usr/local/lib/python3.7/dist-packages/sklearn/base.py\", line 581, in _validate_data\n","    X, y = check_X_y(X, y, **check_params)\n","  File \"/usr/local/lib/python3.7/dist-packages/sklearn/utils/validation.py\", line 976, in check_X_y\n","    estimator=estimator,\n","  File \"/usr/local/lib/python3.7/dist-packages/sklearn/utils/validation.py\", line 726, in check_array\n","    accept_large_sparse=accept_large_sparse,\n","  File \"/usr/local/lib/python3.7/dist-packages/sklearn/utils/validation.py\", line 479, in _ensure_sparse_format\n","    _assert_all_finite(spmatrix.data, allow_nan=force_all_finite == \"allow-nan\")\n","  File \"/usr/local/lib/python3.7/dist-packages/sklearn/utils/validation.py\", line 116, in _assert_all_finite\n","    type_err, msg_dtype if msg_dtype is not None else X.dtype\n","ValueError: Input contains NaN, infinity or a value too large for dtype('float64').\n","\n","  warnings.warn(some_fits_failed_message, FitFailedWarning)\n","/usr/local/lib/python3.7/dist-packages/sklearn/model_selection/_validation.py:372: FitFailedWarning: \n","5 fits failed out of a total of 5.\n","The score on these train-test partitions for these parameters will be set to nan.\n","If these failures are not expected, you can try to debug them by setting error_score='raise'.\n","\n","Below are more details about the failures:\n","--------------------------------------------------------------------------------\n","5 fits failed with the following error:\n","Traceback (most recent call last):\n","  File \"/usr/local/lib/python3.7/dist-packages/sklearn/model_selection/_validation.py\", line 680, in _fit_and_score\n","    estimator.fit(X_train, y_train, **fit_params)\n","  File \"/usr/local/lib/python3.7/dist-packages/sklearn/svm/_classes.py\", line 252, in fit\n","    accept_large_sparse=False,\n","  File \"/usr/local/lib/python3.7/dist-packages/sklearn/base.py\", line 581, in _validate_data\n","    X, y = check_X_y(X, y, **check_params)\n","  File \"/usr/local/lib/python3.7/dist-packages/sklearn/utils/validation.py\", line 976, in check_X_y\n","    estimator=estimator,\n","  File \"/usr/local/lib/python3.7/dist-packages/sklearn/utils/validation.py\", line 726, in check_array\n","    accept_large_sparse=accept_large_sparse,\n","  File \"/usr/local/lib/python3.7/dist-packages/sklearn/utils/validation.py\", line 479, in _ensure_sparse_format\n","    _assert_all_finite(spmatrix.data, allow_nan=force_all_finite == \"allow-nan\")\n","  File \"/usr/local/lib/python3.7/dist-packages/sklearn/utils/validation.py\", line 116, in _assert_all_finite\n","    type_err, msg_dtype if msg_dtype is not None else X.dtype\n","ValueError: Input contains NaN, infinity or a value too large for dtype('float64').\n","\n","  warnings.warn(some_fits_failed_message, FitFailedWarning)\n"]},{"name":"stdout","output_type":"stream","text":["insult f1 score: nan\n","identity_hate f1 score: nan\n","any_label f1 score: nan\n","Average (excluding any) f1 score: nan\n","Elapsed time was 0:01.\n"]},{"name":"stderr","output_type":"stream","text":["/usr/local/lib/python3.7/dist-packages/sklearn/model_selection/_validation.py:372: FitFailedWarning: \n","5 fits failed out of a total of 5.\n","The score on these train-test partitions for these parameters will be set to nan.\n","If these failures are not expected, you can try to debug them by setting error_score='raise'.\n","\n","Below are more details about the failures:\n","--------------------------------------------------------------------------------\n","5 fits failed with the following error:\n","Traceback (most recent call last):\n","  File \"/usr/local/lib/python3.7/dist-packages/sklearn/model_selection/_validation.py\", line 680, in _fit_and_score\n","    estimator.fit(X_train, y_train, **fit_params)\n","  File \"/usr/local/lib/python3.7/dist-packages/sklearn/svm/_classes.py\", line 252, in fit\n","    accept_large_sparse=False,\n","  File \"/usr/local/lib/python3.7/dist-packages/sklearn/base.py\", line 581, in _validate_data\n","    X, y = check_X_y(X, y, **check_params)\n","  File \"/usr/local/lib/python3.7/dist-packages/sklearn/utils/validation.py\", line 976, in check_X_y\n","    estimator=estimator,\n","  File \"/usr/local/lib/python3.7/dist-packages/sklearn/utils/validation.py\", line 726, in check_array\n","    accept_large_sparse=accept_large_sparse,\n","  File \"/usr/local/lib/python3.7/dist-packages/sklearn/utils/validation.py\", line 479, in _ensure_sparse_format\n","    _assert_all_finite(spmatrix.data, allow_nan=force_all_finite == \"allow-nan\")\n","  File \"/usr/local/lib/python3.7/dist-packages/sklearn/utils/validation.py\", line 116, in _assert_all_finite\n","    type_err, msg_dtype if msg_dtype is not None else X.dtype\n","ValueError: Input contains NaN, infinity or a value too large for dtype('float64').\n","\n","  warnings.warn(some_fits_failed_message, FitFailedWarning)\n"]}],"source":["start = time.time()\n","\n","model = LinearSVC(random_state=seed)\n","_ = multi_cv(model, merge_features(training_comments, df, new_features), df_targets)\n","print_time(start)"]},{"cell_type":"markdown","metadata":{"id":"YlhYL_Rc9hXe"},"source":["### Support Vector Machine with Naive Bayes Features"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"kj1pyPGN9hXe"},"outputs":[],"source":["nb = NBFeatures()\n","nb.fit(training_comments, df_targets.any_label)\n","nb_eng = NBFeatures()\n","nb_eng.fit(merge_features(training_comments, df, new_features), df_targets.any_label)\n","# b = nb.transform(training_comments)"]},{"cell_type":"markdown","metadata":{"id":"UfyXWSde9hXg"},"source":["Mini test: Does feature scaling make a difference? Support vector machines are particularly vulnerable to unbalanced features, and I want to check whether scaling after the added step of the Naive Bayes feature transformation makes a difference. "]},{"cell_type":"markdown","metadata":{"id":"Ac1b1q8O9hXo"},"source":["### LightGBM "]},{"cell_type":"code","execution_count":null,"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"elapsed":224858,"status":"ok","timestamp":1664790541861,"user":{"displayName":"akhilvydyula01 dump","userId":"04060569174740840718"},"user_tz":-330},"id":"BI7btDJI9hXq","outputId":"77747028-7b85-4034-e24b-8b659393c978"},"outputs":[{"name":"stderr","output_type":"stream","text":["/usr/local/lib/python3.7/dist-packages/lightgbm/engine.py:430: UserWarning: Found `n_estimators` in params. Will use it instead of argument\n","  warnings.warn(\"Found `{}` in params. Will use it instead of argument\".format(alias))\n"]},{"name":"stdout","output_type":"stream","text":["Elapsed time was 3:44.\n","Final CV F1 score is 0.6599\n"]}],"source":["start = time.time()\n","train_data = lgb.Dataset(training_comments, label=df_targets.any_label.values)\n","params = {\n","    'boosting_type': 'gbdt',\n","    'objective': 'binary',\n","    'verbose': 1,\n","    'num_leaves': 64,\n","    'n_estimators': 100, \n","    'learning_rate': 0.05, \n","    'max_depth': 16,\n","    'n_jobs': -1,\n","    'seed': seed\n","}\n","\n","cv_results = lgb.cv(\n","        params,\n","        train_data,\n","        num_boost_round=100,\n","        nfold=5,\n","        metrics='mae',\n","        early_stopping_rounds=10,\n","        feval=lgb_f1_score\n","        )\n","print_time(start)\n","\n","print(\"Final CV F1 score is %.4f\" % cv_results['f1-mean'][-1])"]},{"cell_type":"markdown","metadata":{"id":"nqtiYQ3R9hXr"},"source":["Elapsed time was 5:05.\n","Final CV F1 score is 0.7470"]},{"cell_type":"markdown","metadata":{"id":"CLfp7xJL9hXs"},"source":["With engineered features. "]},{"cell_type":"code","execution_count":null,"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"elapsed":890962,"status":"ok","timestamp":1664791432769,"user":{"displayName":"akhilvydyula01 dump","userId":"04060569174740840718"},"user_tz":-330},"id":"h6RDCWgn9hXt","outputId":"8e4fa9d5-76e5-4c77-ea58-64575e045c62"},"outputs":[{"name":"stderr","output_type":"stream","text":["/usr/local/lib/python3.7/dist-packages/lightgbm/engine.py:430: UserWarning: Found `n_estimators` in params. Will use it instead of argument\n","  warnings.warn(\"Found `{}` in params. Will use it instead of argument\".format(alias))\n"]},{"name":"stdout","output_type":"stream","text":["Elapsed time was 14:50.\n","Final CV F1 score is 0.7567\n"]}],"source":["start = time.time()\n","train_data = lgb.Dataset(merge_features(training_comments, df, new_features), label=df_targets.any_label.values)\n","params = {\n","    'boosting_type': 'gbdt',\n","    'objective': 'binary',\n","    'verbose': 1,\n","    'num_leaves': 64,\n","    'n_estimators': 500, \n","    'learning_rate': 0.05, \n","    'max_depth': 16,\n","    'n_jobs': -1,\n","    'seed': seed\n","}\n","\n","cv_results = lgb.cv(\n","        params,\n","        train_data,\n","        num_boost_round=100,\n","        nfold=5,\n","        metrics='mae',\n","        #early_stopping_rounds=10,\n","        feval=lgb_f1_score\n","        )\n","print_time(start)\n","\n","print(\"Final CV F1 score is %.4f\" % cv_results['f1-mean'][-1])"]},{"cell_type":"markdown","metadata":{"id":"l-5wf2fk9hXv"},"source":["Elapsed time was 5:04.\n","Final CV F1 score is 0.7573"]},{"cell_type":"markdown","metadata":{"id":"wX5YU--x9hXv"},"source":["# Model Refinement"]},{"cell_type":"markdown","metadata":{"id":"2xIFusM59hXw"},"source":["### Step 1:  Optimize tf-idf max features "]},{"cell_type":"code","execution_count":null,"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"elapsed":12626,"status":"ok","timestamp":1664791445385,"user":{"displayName":"akhilvydyula01 dump","userId":"04060569174740840718"},"user_tz":-330},"id":"-ftZAJyW9hXx","outputId":"1d44fa77-9a94-4efa-ae81-d2be8d94235b"},"outputs":[{"name":"stdout","output_type":"stream","text":["Elapsed time was 0:11.\n","(127656, 10000)\n","0.7693156711613183\n","Elapsed time was 0:02.\n"]}],"source":["start = time.time()\n","comment_vector = TfidfVectorizer(max_features=10000, analyzer='word', #ngram_range=(3, 7), \n","                                 stop_words='english')\n","training_comments = comment_vector.fit_transform(df.comment_text)\n","holdout_comments = comment_vector.transform(holdout.comment_text)\n","submission_comments = comment_vector.transform(df_sub.comment_text)\n","print_time(start)\n","\n","print(training_comments.shape)\n","\n","start = time.time()\n","\n","nb_eng = NBFeatures()\n","nb_eng.fit(merge_features(training_comments, df, new_features), df_targets.any_label)\n","# training_comments = nb_eng.transform(merge_features(training_comments, df, new_features))\n","\n","model = LinearSVC(random_state=seed)\n","\n","score = np.mean(cross_val_score(model, training_comments, df_targets.any_label, scoring='f1', cv=cv))\n","\n","print(score)\n","\n","print_time(start)"]},{"cell_type":"markdown","metadata":{"id":"cBrn23Yg9hXz"},"source":["0.784174616909\n","Elapsed time was 0:33."]},{"cell_type":"code","execution_count":null,"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"elapsed":7718,"status":"ok","timestamp":1664791453065,"user":{"displayName":"akhilvydyula01 dump","userId":"04060569174740840718"},"user_tz":-330},"id":"4vASf0gI9hX0","outputId":"5f656d0f-c60d-4770-97ec-1d92545fc9ee"},"outputs":[{"name":"stdout","output_type":"stream","text":["Elapsed time was 0:05.\n","(127656, 30000)\n","0.7738178806497222\n","Elapsed time was 0:02.\n"]}],"source":["start = time.time()\n","comment_vector = TfidfVectorizer(max_features=30000, analyzer='word', # ngram_range=(3, 7), \n","                                 stop_words='english')\n","training_comments = comment_vector.fit_transform(df.comment_text)\n","#holdout_comments = comment_vector.transform(holdout.comment_text)\n","#submission_comments = comment_vector.transform(df_sub.comment_text)\n","print_time(start)\n","\n","print(training_comments.shape)\n","\n","start = time.time()\n","\n","nb_eng = NBFeatures()\n","nb_eng.fit(merge_features(training_comments, df, new_features), df_targets.any_label)\n","# training_comments = nb_eng.transform(merge_features(training_comments, df, new_features))\n","\n","model = LinearSVC(random_state=seed)\n","\n","score = np.mean(cross_val_score(model, training_comments, df_targets.any_label, scoring='f1', cv=cv))\n","\n","print(score)\n","\n","print_time(start)"]},{"cell_type":"markdown","metadata":{"id":"UzlzVVMm9hX2"},"source":["0.792595764561\n","Elapsed time was 0:21."]},{"cell_type":"code","execution_count":null,"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"elapsed":20666,"status":"ok","timestamp":1664791473691,"user":{"displayName":"akhilvydyula01 dump","userId":"04060569174740840718"},"user_tz":-330},"id":"yAJvGu8i9hX3","outputId":"5a6ecd15-2c9e-4eb1-af54-fa492db00236"},"outputs":[{"name":"stdout","output_type":"stream","text":["Elapsed time was 0:18.\n","(127656, 30000)\n","0.7668741423124089\n","Elapsed time was 0:02.\n"]}],"source":["start = time.time()\n","comment_vector = TfidfVectorizer(max_features=30000, analyzer='word', ngram_range=(1,2), \n","                                 stop_words='english')\n","training_comments = comment_vector.fit_transform(df.comment_text)\n","#holdout_comments = comment_vector.transform(holdout.comment_text)\n","#submission_comments = comment_vector.transform(df_sub.comment_text)\n","print_time(start)\n","\n","print(training_comments.shape)\n","\n","start = time.time()\n","\n","nb_eng = NBFeatures()\n","nb_eng.fit(merge_features(training_comments, df, new_features), df_targets.any_label)\n","# training_comments = nb_eng.transform(merge_features(training_comments, df, new_features))\n","\n","model = LinearSVC(random_state=seed)\n","\n","score = np.mean(cross_val_score(model, training_comments, df_targets.any_label, scoring='f1', cv=cv))\n","\n","print(score)\n","\n","print_time(start)"]},{"cell_type":"markdown","metadata":{"id":"gzSLmAsE9hX4"},"source":["0.787710758788\n","Elapsed time was 0:25."]},{"cell_type":"code","execution_count":null,"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"elapsed":88101,"status":"ok","timestamp":1664791561755,"user":{"displayName":"akhilvydyula01 dump","userId":"04060569174740840718"},"user_tz":-330},"id":"cOJxz1vp9hX5","outputId":"cb21c917-f352-4044-d791-244c0b9f8eb4"},"outputs":[{"name":"stdout","output_type":"stream","text":["Elapsed time was 1:25.\n","(127656, 30000)\n","0.32813592780768525\n","Elapsed time was 0:02.\n"]}],"source":["start = time.time()\n","comment_vector = TfidfVectorizer(max_features=30000, analyzer='word', ngram_range=(2,6), \n","                                 stop_words='english')\n","training_comments = comment_vector.fit_transform(df.comment_text)\n","#holdout_comments = comment_vector.transform(holdout.comment_text)\n","#submission_comments = comment_vector.transform(df_sub.comment_text)\n","print_time(start)\n","\n","print(training_comments.shape)\n","\n","start = time.time()\n","\n","nb_eng = NBFeatures()\n","nb_eng.fit(merge_features(training_comments, df, new_features), df_targets.any_label)\n","# training_comments = nb_eng.transform(merge_features(training_comments, df, new_features))\n","\n","model = LinearSVC(random_state=seed)\n","\n","score = np.mean(cross_val_score(model, training_comments, df_targets.any_label, scoring='f1', cv=cv))\n","\n","print(score)\n","\n","print_time(start)"]},{"cell_type":"markdown","metadata":{"id":"tm4bDubk9hX8"},"source":["0.404212938192\n","Elapsed time was 0:31."]},{"cell_type":"code","execution_count":null,"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"elapsed":269981,"status":"ok","timestamp":1664791831701,"user":{"displayName":"akhilvydyula01 dump","userId":"04060569174740840718"},"user_tz":-330},"id":"JO3d9pW79hX9","outputId":"ec5d5be9-f7ec-47ff-cd5e-d7a4e429f62d"},"outputs":[{"name":"stdout","output_type":"stream","text":["Elapsed time was 3:52.\n","(127656, 20000)\n","0.7728457692028268\n","Elapsed time was 0:36.\n"]}],"source":["start = time.time()\n","comment_vector = TfidfVectorizer(max_features=20000, analyzer='char', ngram_range=(3, 7), \n","                                 stop_words='english')\n","training_comments = comment_vector.fit_transform(df.comment_text)\n","#holdout_comments = comment_vector.transform(holdout.comment_text)\n","#submission_comments = comment_vector.transform(df_sub.comment_text)\n","print_time(start)\n","\n","print(training_comments.shape)\n","\n","start = time.time()\n","\n","nb_eng = NBFeatures()\n","nb_eng.fit(merge_features(training_comments, df, new_features), df_targets.any_label)\n","# training_comments = nb_eng.transform(merge_features(training_comments, df, new_features))\n","\n","model = LinearSVC(random_state=seed)\n","\n","score = np.mean(cross_val_score(model, training_comments, df_targets.any_label, scoring='f1', cv=cv))\n","\n","print(score)\n","\n","print_time(start)"]},{"cell_type":"markdown","metadata":{"id":"bTnXo2_T9hX-"},"source":["0.774235174963\n","Elapsed time was 3:09."]},{"cell_type":"code","execution_count":null,"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"elapsed":296992,"status":"ok","timestamp":1664792128641,"user":{"displayName":"akhilvydyula01 dump","userId":"04060569174740840718"},"user_tz":-330},"id":"c6UdP7lN9hYA","outputId":"2e9c9b16-273a-4988-d2cb-2a3c7a339118"},"outputs":[{"name":"stdout","output_type":"stream","text":["Elapsed time was 4:34.\n","(127656, 10000)\n","0.7843976300873488\n","Elapsed time was 0:22.\n"]}],"source":["start = time.time()\n","word_vectorizer = TfidfVectorizer(max_features=5000, analyzer='word',# ngram_range=(1, 2), \n","                                 stop_words='english')\n","char_vectorizer = TfidfVectorizer(max_features=5000, analyzer='char', ngram_range=(3, 7), \n","                                 stop_words='english')\n","vectorizer = make_union(word_vectorizer, char_vectorizer, n_jobs=-1)\n","training_comments = vectorizer.fit_transform(df.comment_text)\n","print_time(start)\n","print(training_comments.shape)\n","\n","start = time.time()\n","\n","nb_eng = NBFeatures()\n","nb_eng.fit(merge_features(training_comments, df, new_features), df_targets.any_label)\n","# training_comments = nb_eng.transform(merge_features(training_comments, df, new_features))\n","\n","model = LinearSVC(random_state=seed)\n","\n","score = np.mean(cross_val_score(model, training_comments, df_targets.any_label, scoring='f1', cv=cv))\n","\n","print(score)\n","\n","print_time(start)"]},{"cell_type":"markdown","metadata":{"id":"lQ-aG9BW9hYB"},"source":["0.796185423779\n","Elapsed time was 2:56."]},{"cell_type":"code","execution_count":null,"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"elapsed":22817,"status":"ok","timestamp":1664792151411,"user":{"displayName":"akhilvydyula01 dump","userId":"04060569174740840718"},"user_tz":-330},"id":"FdaWTgRq9hYC","outputId":"0881e837-0ba2-47c0-d1a0-c2c9e5c8e0fc"},"outputs":[{"name":"stdout","output_type":"stream","text":["Elapsed time was 0:00.\n","(127656, 10000)\n","0.7843976300873488\n","Elapsed time was 0:22.\n"]}],"source":["start = time.time()\n","word_vectorizer = TfidfVectorizer(max_features=15000, analyzer='word', ngram_range=(1, 2), \n","                                 stop_words='english')\n","char_vectorizer = TfidfVectorizer(max_features=5000, analyzer='char', ngram_range=(3, 7), \n","                                 stop_words='english')\n","vectorizer = make_union(word_vectorizer, char_vectorizer, n_jobs=-1)\n","# training_comments = vectorizer.fit_transform(df.comment_text)\n","print_time(start)\n","print(training_comments.shape)\n","\n","start = time.time()\n","\n","nb_eng = NBFeatures()\n","nb_eng.fit(merge_features(training_comments, df, new_features), df_targets.any_label)\n","# training_comments = nb_eng.transform(merge_features(training_comments, df, new_features))\n","\n","model = LinearSVC(random_state=seed)\n","\n","score = np.mean(cross_val_score(model, training_comments, df_targets.any_label, scoring='f1', cv=cv))\n","\n","print(score)\n","\n","print_time(start)"]},{"cell_type":"markdown","metadata":{"id":"J178JBOD9hYE"},"source":["0.798518003383\n","Elapsed time was 2:07."]},{"cell_type":"code","execution_count":null,"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"elapsed":22931,"status":"ok","timestamp":1664792174280,"user":{"displayName":"akhilvydyula01 dump","userId":"04060569174740840718"},"user_tz":-330},"id":"MfM0AP9I9hYF","outputId":"048e6fbb-fffe-46b6-88cc-116bb5112bda"},"outputs":[{"name":"stdout","output_type":"stream","text":["Elapsed time was 0:00.\n","(127656, 10000)\n","0.7843976300873488\n","Elapsed time was 0:22.\n"]}],"source":["start = time.time()\n","word_vectorizer = TfidfVectorizer(max_features=20000, analyzer='word', ngram_range=(1, 2), \n","                                 stop_words='english')\n","char_vectorizer = TfidfVectorizer(max_features=10000, analyzer='char', ngram_range=(3, 7), \n","                                 stop_words='english')\n","vectorizer = make_union(word_vectorizer, char_vectorizer, n_jobs=-1)\n","# training_comments = vectorizer.fit_transform(df.comment_text)\n","print_time(start)\n","print(training_comments.shape)\n","\n","start = time.time()\n","\n","nb_eng = NBFeatures()\n","nb_eng.fit(merge_features(training_comments, df, new_features), df_targets.any_label)\n","# training_comments = nb_eng.transform(merge_features(training_comments, df, new_features))\n","\n","model = LinearSVC(random_state=seed)\n","\n","score = np.mean(cross_val_score(model, training_comments, df_targets.any_label, scoring='f1', cv=cv))\n","\n","print(score)\n","\n","print_time(start)"]},{"cell_type":"markdown","metadata":{"id":"FHttkOJQ9hYG"},"source":["0.800512138358\n","Elapsed time was 2:36."]},{"cell_type":"code","execution_count":null,"metadata":{"colab":{"background_save":true},"id":"j43gbeQX9hYH","outputId":"1dd7057e-58be-4cf9-8bfe-dc0311ec5f06"},"outputs":[{"name":"stdout","output_type":"stream","text":["Elapsed time was 2:15.\n","(127656, 30000)\n","0.7907162252541873\n","Elapsed time was 0:24.\n"]}],"source":["start = time.time()\n","word_vectorizer = TfidfVectorizer(max_features=20000, analyzer='word', ngram_range=(1, 2), \n","                                 stop_words='english')\n","char_vectorizer = TfidfVectorizer(max_features=10000, analyzer='char', ngram_range=(3, 5), \n","                                 stop_words='english')\n","vectorizer = make_union(word_vectorizer, char_vectorizer, n_jobs=-1)\n","training_comments = vectorizer.fit_transform(df.comment_text)\n","print_time(start)\n","print(training_comments.shape)\n","\n","start = time.time()\n","\n","nb_eng = NBFeatures()\n","nb_eng.fit(merge_features(training_comments, df, new_features), df_targets.any_label)\n","# training_comments = nb_eng.transform(merge_features(training_comments, df, new_features))\n","\n","model = LinearSVC(random_state=seed)\n","\n","score = np.mean(cross_val_score(model, training_comments, df_targets.any_label, scoring='f1', cv=cv))\n","\n","print(score)\n","\n","print_time(start)"]},{"cell_type":"markdown","metadata":{"id":"xJH7Vy7Q9hYK"},"source":["0.80146407813\n","Elapsed time was 2:48."]},{"cell_type":"markdown","metadata":{"id":"EXeqw8yr9hYL"},"source":["### Step 2: Optimize NB Feature Weight"]},{"cell_type":"markdown","metadata":{"id":"vGmKorGF9hYM"},"source":["There appears to be an issue with the consistency of scores here. The variation in training time suggests that the support vector machine algorithm is struggling with a too-large range of input values. Using the the last and best training_comments from the cell above. "]},{"cell_type":"code","execution_count":32,"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"Jc1FWbl99hYN","scrolled":false,"executionInfo":{"status":"ok","timestamp":1664792688623,"user_tz":-330,"elapsed":211618,"user":{"displayName":"akhilvydyula01 dump","userId":"04060569174740840718"}},"outputId":"9b120bab-9518-4ae8-e9f5-fb9154031669"},"outputs":[{"output_type":"stream","name":"stdout","text":["Elapsed time was 2:14.\n","(127656, 30000)\n","**********************\n","For epsilon 0.100000\n","Epsilon 0.100000 score: 0.7907\n","Elapsed time was 0:25.\n","**********************\n","For epsilon 0.200000\n","Epsilon 0.200000 score: 0.7907\n","Elapsed time was 0:24.\n","**********************\n","For epsilon 0.300000\n","Epsilon 0.300000 score: 0.7907\n","Elapsed time was 0:24.\n","**********************\n","For epsilon 0.400000\n","Epsilon 0.400000 score: 0.7907\n","Elapsed time was 0:24.\n","**********************\n","For epsilon 0.500000\n","Epsilon 0.500000 score: 0.7907\n","Elapsed time was 0:24.\n","**********************\n","For epsilon 0.600000\n","Epsilon 0.600000 score: 0.7907\n","Elapsed time was 0:24.\n","**********************\n","For epsilon 0.700000\n","Epsilon 0.700000 score: 0.7907\n","Elapsed time was 0:24.\n","**********************\n","For epsilon 0.800000\n","Epsilon 0.800000 score: 0.7907\n","Elapsed time was 0:24.\n","**********************\n","For epsilon 0.900000\n","Epsilon 0.900000 score: 0.7907\n","Elapsed time was 0:24.\n"]}],"source":["start = time.time()\n","word_vectorizer = TfidfVectorizer(max_features=20000, analyzer='word', ngram_range=(1, 2), \n","                                 stop_words='english')\n","char_vectorizer = TfidfVectorizer(max_features=10000, analyzer='char', ngram_range=(3, 5), \n","                                 stop_words='english')\n","vectorizer = make_union(word_vectorizer, char_vectorizer, n_jobs=-1)\n","training_comments = vectorizer.fit_transform(df.comment_text)\n","print_time(start)\n","print(training_comments.shape)\n","\n","for i in range(1,10):\n","    start = time.time()\n","    sc = StandardScaler(with_mean=False)\n","    epsilon = i/10\n","    print('**********************')\n","    print('For epsilon %f' % epsilon)\n","    nb_temp = NBFeatures(epsilon=epsilon)\n","    nb_temp.fit(merge_features(training_comments, df, new_features), df_targets.any_label)\n","    # input_data = nb_temp.transform(merge_features(training_comments, df, new_features))\n","    model = LinearSVC(random_state=seed)\n","    score = np.mean(cross_val_score(model, training_comments, df_targets.any_label, scoring='f1', cv=cv))\n","    print('Epsilon %f score: %.4f' % (epsilon, score))\n","    print_time(start)"]},{"cell_type":"markdown","metadata":{"id":"_TJNzVqm9hYP"},"source":["### Step 3: SVM Parameter Tuning "]},{"cell_type":"code","execution_count":33,"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"8UFBbBkP9hYQ","executionInfo":{"status":"ok","timestamp":1664792829894,"user_tz":-330,"elapsed":141273,"user":{"displayName":"akhilvydyula01 dump","userId":"04060569174740840718"}},"outputId":"f741790e-c49f-4e18-ddb2-728dfec762e5"},"outputs":[{"output_type":"stream","name":"stdout","text":["Elapsed time was 2:17.\n","(127656, 30000)\n"]}],"source":["start = time.time()\n","word_vectorizer = TfidfVectorizer(max_features=20000, analyzer='word', ngram_range=(1, 2), \n","                                 stop_words='english')\n","char_vectorizer = TfidfVectorizer(max_features=10000, analyzer='char', ngram_range=(3, 5), \n","                                 stop_words='english')\n","vectorizer = make_union(word_vectorizer, char_vectorizer, n_jobs=-1)\n","training_comments = vectorizer.fit_transform(df.comment_text)\n","print_time(start)\n","print(training_comments.shape)\n","\n","# Reset NB feature transformer epsilon value\n","nb_eng = NBFeatures()\n","nb_eng.fit(merge_features(training_comments, df, new_features), df_targets.any_label)\n","# input_data = nb_eng.transform(merge_features(training_comments, df, new_features))"]},{"cell_type":"code","execution_count":null,"metadata":{"colab":{"background_save":true},"id":"S_dITrvO9hYS"},"outputs":[],"source":[]},{"cell_type":"markdown","metadata":{"id":"hLz43XWN9hYY"},"source":["### Optimimum Model"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"3NGTTMKz9hYY"},"outputs":[],"source":["# TF-IDF Vectorization\n","start = time.time()\n","word_vectorizer = TfidfVectorizer(max_features=200, analyzer='word', ngram_range=(1, 2), \n","                                 stop_words='english')\n","char_vectorizer = TfidfVectorizer(max_features=10000, analyzer='char', ngram_range=(3, 5), \n","                                 stop_words='english')\n","vectorizer = make_union(word_vectorizer, char_vectorizer, n_jobs=-1)\n","\n","# Fit to and transform input data\n","X_train = vectorizer.fit_transform(df.comment_text)\n","X_test = vectorizer.transform(holdout.comment_text)\n","\n","# Name training target data\n","y_train = df_targets.any_label\n","y_test = holdout_targets.any_label\n","\n","# Create and fit NB Feature extractor \n","nb = NBFeatures()\n","nb.fit(X_train, y_train)\n","\n","# Tranform input data\n","X_train = nb.transform(X_train)\n","X_test = nb.transform(X_test)\n","\n","print_time(start)\n","\n","# Define model and fit to data \n","start = time.time()\n","model = LinearSVC(random_state=seed, C=0.5)\n","model.fit(X_train, y_train)\n","\n","print_time(start)"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"hAyZmH-a9hYa"},"outputs":[],"source":["y_pred = model.predict(X_test)"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"-sf-yvbf9hYb"},"outputs":[],"source":["tn, fp, fn, tp = confusion_matrix(y_test, y_pred).ravel()\n","\n","acc = (tp+tn)/(tn+fn+tp+fp)\n","print(\"True Positives: %d\" % tp)\n","print(\"False Positives: %d\" % fp)\n","print(\"True Negatives: %d\" % tn)\n","print(\"False Negatives: %d\" % fn)\n","print(\"Precision: %.4f\" % (tp/(tp+fp)))\n","print(\"Recall: %.4f\" % (tp/(tp+fn)))\n","print(\"F1 Score: %.4f\" % f1_score(y_test, y_pred))\n","print(\"Total Accuracy: %.2f%%\" % acc)"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"sLcCgszk9hYc"},"outputs":[],"source":["cm = confusion_matrix(y_test, y_pred)\n","def cm_heatmap(arr, title):\n","    \"\"\"Internal. Only called by scoring function.\"\"\"\n","    plt.figure('cm_heatmap', figsize=(10,10))\n","    plt.title(title + ' confusion matrix')\n","    sns.heatmap(arr, square=True, annot=True, cmap='YlOrRd', fmt='g', cbar=False)\n","    plt.xlabel(\"Truth\")\n","    plt.ylabel(\"Prediction\")\n","    #sns.set(font_scale=3)\n","    plt.show()\n","cm_heatmap(cm, \"Toxic Comments\")"]}],"metadata":{"accelerator":"GPU","colab":{"collapsed_sections":["kaYNMVqj5yxo"],"toc_visible":true,"provenance":[]},"kernelspec":{"display_name":"Python [conda env:py35]","language":"python","name":"conda-env-py35-py"},"language_info":{"codemirror_mode":{"name":"ipython","version":3},"file_extension":".py","mimetype":"text/x-python","name":"python","nbconvert_exporter":"python","pygments_lexer":"ipython3","version":"3.5.5"}},"nbformat":4,"nbformat_minor":0}